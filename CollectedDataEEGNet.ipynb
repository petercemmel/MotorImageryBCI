{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVyimJRhQVeI",
        "outputId": "bf94aaf2-7452-4516-be41-a26a762752da"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI2XXycMXq5Y",
        "outputId": "0b50ca61-3490-4b0d-d539-5de58a9933bd"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'content/drive/MyDrive/MotorImagery-master')\n",
        "\n",
        "import IPython\n",
        "from os import system\n",
        "%clear"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[H\u001b[2J"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osf6WA9Zzrbs"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import SpatialDropout2D\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.layers import Input, Flatten\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras import backend as K\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def EEGNet(nb_classes = 4, Chans = 32, Samples = 1024, \n",
        "             dropoutRate = 0.5, kernLength = 32, F1 = 8, \n",
        "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
        "    \"\"\" Keras Implementation of EEGNet\n",
        "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
        "    Note that this implements the newest version of EEGNet and NOT the earlier\n",
        "    version (version v1 and v2 on arxiv). We strongly recommend using this\n",
        "    architecture as it performs much better and has nicer properties than\n",
        "    our earlier version....\n",
        "    \n",
        "    Inputs:\n",
        "        \n",
        "      nb_classes      : int, number of classes to classify\n",
        "      Chans, Samples  : number of channels and time points in the EEG data\n",
        "      dropoutRate     : dropout fraction\n",
        "      kernLength      : length of temporal convolution in first layer. We found\n",
        "                        that setting this to be half the sampling rate worked\n",
        "                        well in practice. For the SMR dataset in particular\n",
        "                        since the data was high-passed at 4Hz we used a kernel\n",
        "                        length of 32.     \n",
        "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
        "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
        "      D               : number of spatial filters to learn within each temporal #possibly decrease to 1\n",
        "                        convolution. Default: D = 2\n",
        "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
        "    \"\"\"\n",
        "    \n",
        "    if dropoutType == 'SpatialDropout2D':\n",
        "        dropoutType = SpatialDropout2D\n",
        "    elif dropoutType == 'Dropout':\n",
        "        dropoutType = Dropout\n",
        "    else:\n",
        "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
        "                         'or Dropout, passed as a string.')\n",
        "    \n",
        "    input1   = Input(shape = (1, Chans, Samples))\n",
        "\n",
        "    ##################################################################\n",
        "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
        "                                   input_shape = (1, Chans, Samples),\n",
        "                                   use_bias = False)(input1)\n",
        "    block1       = BatchNormalization(axis = 1)(block1)\n",
        "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
        "                                   depth_multiplier = D,\n",
        "                                   depthwise_constraint = max_norm(1.))(block1)\n",
        "    block1       = BatchNormalization(axis = 1)(block1)\n",
        "    block1       = Activation('elu')(block1)\n",
        "    block1       = AveragePooling2D((1, 4))(block1)\n",
        "    block1       = dropoutType(dropoutRate)(block1)\n",
        "    \n",
        "    block2       = SeparableConv2D(F2, (1, 16),\n",
        "                                   use_bias = False, padding = 'same')(block1)\n",
        "    block2       = BatchNormalization(axis = 1)(block2)\n",
        "    block2       = Activation('elu')(block2)\n",
        "    block2       = AveragePooling2D((1, 8))(block2)\n",
        "    block2       = dropoutType(dropoutRate)(block2)\n",
        "        \n",
        "    flatten      = Flatten(name = 'flatten')(block2)\n",
        "    \n",
        "    dense        = Dense(nb_classes, name = 'dense', \n",
        "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
        "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
        "    \n",
        "    return Model(inputs=input1, outputs=softmax)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  # need these for ShallowConvNet\n",
        "def square(x):\n",
        "    return K.square(x)\n",
        "\n",
        "def log(x):\n",
        "    return K.log(K.clip(x, min_value = 1e-7, max_value = 10000))   \n",
        "\n",
        "\n",
        "def ShallowConvNet(nb_classes, Chans = 64, Samples = 128, dropoutRate = 0.5):\n",
        "    \"\"\" Keras implementation of the Shallow Convolutional Network as described\n",
        "    in Schirrmeister et. al. (2017), Human Brain Mapping.\n",
        "    \n",
        "    Assumes the input is a 2-second EEG signal sampled at 128Hz. Note that in \n",
        "    the original paper, they do temporal convolutions of length 25 for EEG\n",
        "    data sampled at 250Hz. We instead use length 13 since the sampling rate is \n",
        "    roughly half of the 250Hz which the paper used. The pool_size and stride\n",
        "    in later layers is also approximately half of what is used in the paper.\n",
        "    \n",
        "    Note that we use the max_norm constraint on all convolutional layers, as \n",
        "    well as the classification layer. We also change the defaults for the\n",
        "    BatchNormalization layer. We used this based on a personal communication \n",
        "    with the original authors.\n",
        "    \n",
        "                     ours        original paper\n",
        "    pool_size        1, 35       1, 75\n",
        "    strides          1, 7        1, 15\n",
        "    conv filters     1, 13       1, 25    \n",
        "    \n",
        "    Note that this implementation has not been verified by the original \n",
        "    authors. We do note that this implementation reproduces the results in the\n",
        "    original paper with minor deviations. \n",
        "    \"\"\"\n",
        "\n",
        "    # start the model\n",
        "    input_main   = Input((1, Chans, Samples))\n",
        "    block1       = Conv2D(40, (1, 13), \n",
        "                                 input_shape=(1, Chans, Samples),\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
        "    block1       = Conv2D(40, (Chans, 1), use_bias=False, \n",
        "                          kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
        "    block1       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block1)\n",
        "    block1       = Activation(square)(block1)\n",
        "    block1       = AveragePooling2D(pool_size=(1, 35), strides=(1, 7))(block1)\n",
        "    block1       = Activation(log)(block1)\n",
        "    block1       = Dropout(dropoutRate)(block1)\n",
        "    flatten      = Flatten()(block1)\n",
        "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
        "    softmax      = Activation('softmax')(dense)\n",
        "    \n",
        "    return Model(inputs=input_main, outputs=softmax)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQadQ3n20FoV"
      },
      "source": [
        "# CollectedData TrainEEG\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "# channels first\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "# establish variables\n",
        "subject_id = [1]\n",
        "fs = 256                            # sampling rate 256Hz\n",
        "num_classes = 4                     # four classes\n",
        "num_channels = 32                    # 8 channels\n",
        "tot_samples = 1024                  # 1000 total time points\n",
        "window = [500]                 # number of samples in window we're looking at\n",
        "\n",
        "# Modify this value for windowed\n",
        "num_samples = 500\n",
        "\n",
        "\n",
        "output_path = '/content/drive/MyDrive/MotorImagery-master/Data/CollectedData/ArdoWet/500Windowed/0/EEG_Train'\n",
        "data_path = '/content/drive/MyDrive/MotorImagery-master/Data/CollectedData/ArdoWet/500Windowed/0'\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    os.mkdir(output_path)\n",
        "    print('created %s',output_path)\n",
        "\n",
        "##########\n",
        "# Train regular\n",
        "##########\n",
        "for n in subject_id:\n",
        "    print()\n",
        "    print(\"Subject: \", n)\n",
        "    print()\n",
        "      \n",
        "   \n",
        "    # Load data\n",
        "    y_train = np.load(data_path+\"/y_train_\"+str(n)+\".npy\")\n",
        "    X_train = np.load(data_path+\"/X_train_\"+str(n)+\".npy\")\n",
        "\n",
        "    #print(\"y train, size:\", len(y_train))\n",
        "    #print(y_train)\n",
        "\n",
        "\n",
        "    print(\"x train, shape:\", X_train.shape)\n",
        "\n",
        "\n",
        "    trials, dim1, dim2 = X_train.shape\n",
        "\n",
        "    print(trials)\n",
        "    print(dim1)\n",
        "    print(dim2)\n",
        "  \n",
        "\n",
        "\n",
        "    if (dim1 == num_channels):\n",
        "        # chans = dim1, samples = dim2\n",
        "        X_train = np.reshape(X_train, (trials, 1 , dim1, dim2))\n",
        "    elif (dim1 == num_samples):\n",
        "        # samples = dim1, chans = dim2\n",
        "        X_train = np.transpose(X_train, (0, 2, 1))\n",
        "        X_train = np.reshape(X_train, (trials, 1, dim2, dim1))\n",
        "        X_train= np.array(X_train)\n",
        "    else:\n",
        "        print(\"Error\")\n",
        "    \n",
        "    y_val = np.load(data_path+\"/y_val_\"+str(n)+\".npy\")\n",
        "    X_val = np.load(data_path+\"/X_val_\"+str(n)+\".npy\")\n",
        "    trials, dim1, dim2 = X_val.shape\n",
        "    if (dim1 == num_channels):\n",
        "        # chans = dim1, samples = dim2\n",
        "        X_val = np.reshape(X_val, (trials, 1 , dim1, dim2))\n",
        "    elif (dim1 == num_samples):\n",
        "        # samples = dim1, chans = dim2\n",
        "        X_val = np.transpose(X_val, (0, 2, 1))\n",
        "        X_val = np.reshape(X_val, (trials, 1, dim2, dim1))\n",
        "    else:\n",
        "        print(\"Error\")\n",
        "\n",
        "    # Build Model\n",
        "    model = EEGNet(Samples = num_samples)\n",
        "    model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['acc'])\n",
        "    model.summary()\n",
        "    # Add Checkpoint\n",
        "    checkpointer = ModelCheckpoint(filepath= output_path+\"/subject\"+str(n), \n",
        "                                   monitor='val_acc',\n",
        "                                   mode='max',\n",
        "                                   verbose=1,\n",
        "                                   save_best_only=True)\n",
        "    # Train and save model\n",
        "    print(\"X_train: \", X_train.shape)\n",
        "    print(\"y_train: \", y_train.shape)\n",
        "    print(\"X_val: \", X_val.shape)\n",
        "    print(\"y_val: \", y_val.shape)\n",
        "    #print(y_val[0:9])\n",
        "    history = model.fit(X_train,\n",
        "                         y_train,\n",
        "                         batch_size=16,\n",
        "                         validation_data=(X_val, y_val),\n",
        "                         epochs=500,\n",
        "                         callbacks=[checkpointer])\n",
        "    history.history.keys()\n",
        "    history.history\n",
        "\n",
        "    # labels, predictions, num_classes=None, weights=None, dtype=tf.dtypes.int32,\n",
        "    # name=None)\n",
        "\n",
        "    np.save(output_path+\"/subject\"+str(n)+\"_acc\", history.history[\"acc\"])\n",
        "    np.save(output_path+\"/subject\"+str(n)+\"_loss\", history.history[\"loss\"])\n",
        "    np.save(output_path+\"/subject\"+str(n)+\"_val_acc\", history.history[\"val_acc\"])\n",
        "    np.save(output_path+\"/subject\"+str(n)+\"_val_loss\", history.history[\"val_loss\"])\n",
        "      \n",
        "    print()\n",
        "    print()\n",
        "    \n",
        "print()\n",
        "print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoVOjQ_qQgs9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48dd0299-ccc2-4788-8fb3-42f2a9bbdd1e"
      },
      "source": [
        "# TestEEG\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# channels first\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "# Define variables\n",
        "subject_id = [1]      \n",
        "num_channels = 32                 # 8 channels\n",
        "tot_samples = 1000                # 1000 total samples \n",
        "fs = 250                          # sampling rate 250Hz\n",
        "\n",
        "# define file paths\n",
        "\n",
        "data_path = '/content/drive/MyDrive/MotorImagery-master/Data/CollectedData/ArdoWet/500Windowed/0'\n",
        "weights_path = '/content/drive/MyDrive/MotorImagery-master/Data/CollectedData/ArdoWet/500Windowed/0/EEG_Train'\n",
        "num_samples = 500\n",
        "accuracy=[]\n",
        "\n",
        "\n",
        "\n",
        "# loop through subjects \n",
        "for n in subject_id:\n",
        "    # load testing data\n",
        "    X_test = np.load(data_path+\"/X_test_\"+str(n)+\".npy\")\n",
        "    y_test = np.load(data_path+\"/y_test_\"+str(n)+\".npy\")\n",
        "    # Reshape Xtest data into (trials, 1, chans, samples)\n",
        "    trials, dim1, dim2 = X_test.shape\n",
        "    if (dim1 == num_channels):\n",
        "        # chans = dim1, samples = dim2\n",
        "        X_test = np.reshape(X_test, (trials, 1 , dim1, dim2))\n",
        "    elif (dim1 == num_samples):\n",
        "        # samples = dim1, chans = dim2\n",
        "        X_test = np.transpose(X_test, (0, 2, 1))\n",
        "        X_test = np.reshape(X_test, (trials, 1, dim2, dim1))\n",
        "    else:\n",
        "        print(\"Error\")\n",
        "        \n",
        "    # Build Model\n",
        "    model = EEGNet(Samples = num_samples)\n",
        "    model.compile(optimizer='adam',\n",
        "                   loss='sparse_categorical_crossentropy',\n",
        "                    metrics=['accuracy'])\n",
        "    \n",
        "    # Load trained model\n",
        "    model.load_weights(weights_path+\"/subject\"+str(n))\n",
        "            \n",
        "    # Evaluate\n",
        "    loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "    pred_array = model.predict(X_test)\n",
        "    \n",
        "    accuracy.append(acc)\n",
        "    print(\"Subject \", n, \": {:5.2f}%\".format(100*acc))\n",
        "    print()\n",
        "\n",
        "print()\n",
        "print(\"Mean Accuracy of EEGNet is\",np.mean(accuracy)*100,\"%\")\n",
        "print(\"Standard Deviation of EEGNet is\",np.std(accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 1.1194 - accuracy: 0.4500\n",
            "Subject  1 : 45.00%\n",
            "\n",
            "\n",
            "Mean Accuracy of EEGNet is 44.999998807907104 %\n",
            "Standard Deviation of EEGNet is 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBVxerrwdKjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40538e3f-19a1-4083-f292-f27a75bb5647"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "predictions = np.zeros(len(pred_array[:,0]))\n",
        "for i in range(len(pred_array[:,0])):\n",
        "  predictions[i] = np.argmax(pred_array[i])\n",
        "\n",
        "cm = confusion_matrix(np.ravel(y_test), predictions)\n",
        "tasks = ['left','right','down','up']\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 9 3 0]\n",
            " [4 5 1 2]\n",
            " [1 4 2 0]\n",
            " [1 4 0 4]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4-class system\n",
        "predictions = np.zeros(len(pred_array[:,0]))\n",
        "for i in range(len(pred_array[:,0])):\n",
        "  predictions[i] = np.argmax(pred_array[i])\n",
        "\n",
        "print(' ')\n",
        "print('The order of predicted labels is:')\n",
        "print(predictions)\n",
        "\n",
        "#predictions2 = np.where(predictions == 0, 1 , predictions)\n",
        "#print(predictions2)\n",
        "\n",
        "#Alphabet table\n",
        "table = { '[0. 0. 0.]': 'A', '[0. 0. 1.]': 'B', '[0. 0. 2.]': 'C', '[0. 1. 0.]': 'D', '[0. 1. 1.]': 'E', '[0. 1. 2.]': 'F',\n",
        "          '[0. 2. 0.]': 'G', '[0. 2. 1.]': 'H', '[0. 2. 2.]': 'I', '[1. 0. 0.]': 'J', '[1. 0. 1.]': 'K', '[1. 0. 2.]': 'L',\n",
        "          '[1. 1. 0.]': 'M', '[1. 1. 1.]': 'N', '[1. 1. 2.]': 'O', '[1. 2. 0.]': 'P', '[1. 2. 1.]': 'Q', '[1. 2. 2.]': 'R',\n",
        "          '[2. 0. 0.]': 'S', '[2. 0. 1.]': 'T', '[2. 0. 2.]': 'U', '[2. 1. 0.]': 'V', '[2. 1. 1.]': 'W', '[2. 1. 2.]': 'X',\n",
        "          '[2. 2. 0.]': 'Y', '[2. 2. 1.]': 'Z', '[2. 2. 2.]': ' ', '[3. 0. 0.]': '1', '[3. 0. 1.]': '2', '[3. 0. 2.]': '3',\n",
        "          '[3. 1. 0.]': '4', '[3. 1. 1.]': '5', '[3. 1. 2.]': '6', '[3. 2. 0.]': '7', '[3. 2. 1.]': '8', '[3. 2. 2.]': '9',\n",
        "}\n",
        "print(' ')\n",
        "print('The order of viable codons is:')\n",
        "letter = \"\"\n",
        "for i in range(0, len(predictions)- len(predictions)%3,3):\n",
        "    codon = predictions[i:i + 3]\n",
        "    if codon[1] == 3 or codon[2] == 3:\n",
        "      continue\n",
        "    print(codon)\n",
        "    letter += table[str(codon)]\n",
        "print(' ')\n",
        "print('The predicted text is:')\n",
        "print(letter)"
      ],
      "metadata": {
        "id": "EgtfCXCc_mTA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab2fa918-fbfc-4afd-af3d-6aa8594cd09f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "The order of predicted labels is:\n",
            "[3. 3. 2. 3. 2. 0. 3. 1. 1. 0. 0. 0. 2. 0. 0. 1. 0. 0. 2. 0. 0. 2. 3. 1.\n",
            " 3. 2. 0. 1. 0. 1. 2. 3. 0. 2. 3. 3. 2. 1. 2. 3. 1. 2. 0. 2. 3. 0. 3. 2.]\n",
            " \n",
            "The order of viable codons is:\n",
            "[3. 2. 0.]\n",
            "[3. 1. 1.]\n",
            "[0. 0. 0.]\n",
            "[2. 0. 0.]\n",
            "[1. 0. 0.]\n",
            "[2. 0. 0.]\n",
            "[3. 2. 0.]\n",
            "[1. 0. 1.]\n",
            "[2. 1. 2.]\n",
            "[3. 1. 2.]\n",
            " \n",
            "The predicted text is:\n",
            "75ASJS7KX6\n"
          ]
        }
      ]
    }
  ]
}