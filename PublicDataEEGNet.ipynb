{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVrDVtUH0oD0",
        "outputId": "0de982b9-b577-4919-b772-5328b33578e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar  2 01:36:20 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI2XXycMXq5Y",
        "outputId": "989601b7-9f38-40e9-8d70-1e49a40f9ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[H\u001b[2J"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'content/drive/MyDrive/MotorImagery-master')\n",
        "\n",
        "import IPython\n",
        "from os import system\n",
        "%clear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbo5_NieQIpZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Permute, Dropout\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
        "from tensorflow.keras.layers import SeparableConv2D, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import SpatialDropout2D\n",
        "from tensorflow.keras.regularizers import l1_l2\n",
        "from tensorflow.keras.layers import Input, Flatten\n",
        "from tensorflow.keras.constraints import max_norm\n",
        "from tensorflow.keras import backend as K\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def EEGNet(nb_classes = 4, Chans = 22, Samples = 256, \n",
        "             dropoutRate = 0.5, kernLength = 32, F1 = 8, \n",
        "             D = 2, F2 = 16, norm_rate = 0.25, dropoutType = 'Dropout'):\n",
        "    \"\"\" Keras Implementation of EEGNet\n",
        "    http://iopscience.iop.org/article/10.1088/1741-2552/aace8c/meta\n",
        "    Note that this implements the newest version of EEGNet and NOT the earlier\n",
        "    version (version v1 and v2 on arxiv). We strongly recommend using this\n",
        "    architecture as it performs much better and has nicer properties than\n",
        "    our earlier version....\n",
        "    \n",
        "    Inputs:\n",
        "        \n",
        "      nb_classes      : int, number of classes to classify\n",
        "      Chans, Samples  : number of channels and time points in the EEG data\n",
        "      dropoutRate     : dropout fraction\n",
        "      kernLength      : length of temporal convolution in first layer. We found\n",
        "                        that setting this to be half the sampling rate worked\n",
        "                        well in practice. For the SMR dataset in particular\n",
        "                        since the data was high-passed at 4Hz we used a kernel\n",
        "                        length of 32.     \n",
        "      F1, F2          : number of temporal filters (F1) and number of pointwise\n",
        "                        filters (F2) to learn. Default: F1 = 8, F2 = F1 * D. \n",
        "      D               : number of spatial filters to learn within each temporal #possibly decrease to 1\n",
        "                        convolution. Default: D = 2\n",
        "      dropoutType     : Either SpatialDropout2D or Dropout, passed as a string.\n",
        "    \"\"\"\n",
        "    \n",
        "    if dropoutType == 'SpatialDropout2D':\n",
        "        dropoutType = SpatialDropout2D\n",
        "    elif dropoutType == 'Dropout':\n",
        "        dropoutType = Dropout\n",
        "    else:\n",
        "        raise ValueError('dropoutType must be one of SpatialDropout2D '\n",
        "                         'or Dropout, passed as a string.')\n",
        "    \n",
        "    input1   = Input(shape = (1, Chans, Samples))\n",
        "\n",
        "    ##################################################################\n",
        "    block1       = Conv2D(F1, (1, kernLength), padding = 'same',\n",
        "                                   input_shape = (1, Chans, Samples),\n",
        "                                   use_bias = False)(input1)\n",
        "    block1       = BatchNormalization(axis = 1)(block1)\n",
        "    block1       = DepthwiseConv2D((Chans, 1), use_bias = False, \n",
        "                                   depth_multiplier = D,\n",
        "                                   depthwise_constraint = max_norm(1.))(block1)\n",
        "    block1       = BatchNormalization(axis = 1)(block1)\n",
        "    block1       = Activation('elu')(block1)\n",
        "    block1       = AveragePooling2D((1, 4))(block1)\n",
        "    block1       = dropoutType(dropoutRate)(block1)\n",
        "    \n",
        "    block2       = SeparableConv2D(F2, (1, 16),\n",
        "                                   use_bias = False, padding = 'same')(block1)\n",
        "    block2       = BatchNormalization(axis = 1)(block2)\n",
        "    block2       = Activation('elu')(block2)\n",
        "    block2       = AveragePooling2D((1, 8))(block2)\n",
        "    block2       = dropoutType(dropoutRate)(block2)\n",
        "        \n",
        "    flatten      = Flatten(name = 'flatten')(block2)\n",
        "    \n",
        "    dense        = Dense(nb_classes, name = 'dense', \n",
        "                         kernel_constraint = max_norm(norm_rate))(flatten)\n",
        "    softmax      = Activation('softmax', name = 'softmax')(dense)\n",
        "    \n",
        "    return Model(inputs=input1, outputs=softmax)\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "  # need these for ShallowConvNet\n",
        "def square(x):\n",
        "    return K.square(x)\n",
        "\n",
        "def log(x):\n",
        "    return K.log(K.clip(x, min_value = 1e-7, max_value = 10000))   \n",
        "\n",
        "\n",
        "def ShallowConvNet(nb_classes, Chans = 64, Samples = 128, dropoutRate = 0.5):\n",
        "    \"\"\" Keras implementation of the Shallow Convolutional Network as described\n",
        "    in Schirrmeister et. al. (2017), Human Brain Mapping.\n",
        "    \n",
        "    Assumes the input is a 2-second EEG signal sampled at 128Hz. Note that in \n",
        "    the original paper, they do temporal convolutions of length 25 for EEG\n",
        "    data sampled at 250Hz. We instead use length 13 since the sampling rate is \n",
        "    roughly half of the 250Hz which the paper used. The pool_size and stride\n",
        "    in later layers is also approximately half of what is used in the paper.\n",
        "    \n",
        "    Note that we use the max_norm constraint on all convolutional layers, as \n",
        "    well as the classification layer. We also change the defaults for the\n",
        "    BatchNormalization layer. We used this based on a personal communication \n",
        "    with the original authors.\n",
        "    \n",
        "                     ours        original paper\n",
        "    pool_size        1, 35       1, 75\n",
        "    strides          1, 7        1, 15\n",
        "    conv filters     1, 13       1, 25    \n",
        "    \n",
        "    Note that this implementation has not been verified by the original \n",
        "    authors. We do note that this implementation reproduces the results in the\n",
        "    original paper with minor deviations. \n",
        "    \"\"\"\n",
        "\n",
        "    # start the model\n",
        "    input_main   = Input((1, Chans, Samples))\n",
        "    block1       = Conv2D(40, (1, 13), \n",
        "                                 input_shape=(1, Chans, Samples),\n",
        "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
        "    block1       = Conv2D(40, (Chans, 1), use_bias=False, \n",
        "                          kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
        "    block1       = BatchNormalization(axis=1, epsilon=1e-05, momentum=0.1)(block1)\n",
        "    block1       = Activation(square)(block1)\n",
        "    block1       = AveragePooling2D(pool_size=(1, 35), strides=(1, 7))(block1)\n",
        "    block1       = Activation(log)(block1)\n",
        "    block1       = Dropout(dropoutRate)(block1)\n",
        "    flatten      = Flatten()(block1)\n",
        "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
        "    softmax      = Activation('softmax')(dense)\n",
        "    \n",
        "    return Model(inputs=input_main, outputs=softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdpZJBxvTED-"
      },
      "outputs": [],
      "source": [
        "#TrainEEG\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "print(device_lib.list_local_devices())\n",
        "\n",
        "# channels first\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "# establish variables\n",
        "subject_id = np.arange(0,9)        # 9 subjects\n",
        "fs = 250                            # sampling rate 250Hz\n",
        "num_classes = 4                     # four classes\n",
        "num_channels = 22                   # 22 channels\n",
        "tot_samples = 1000                  # 1000 total time points\n",
        "window = [500]  # number of samples in window we're looking at\n",
        "\n",
        "# path to files\n",
        "num_samples = 500\n",
        "input_path = '/content/drive/MyDrive/MotorImagery-master/Data/PublicData/500Windowed/0'\n",
        "output_path = '/content/drive/MyDrive/MotorImagery-master/Data/PublicData/500Windowed/0/EEG_train'\n",
        "data_path = '/content/drive/MyDrive/MotorImagery-master/Data/PublicData/500Windowed/0'\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "     os.mkdir(output_path)\n",
        "     print('created %s',output_path)\n",
        "\n",
        "##########\n",
        "# Train regular\n",
        "##########\n",
        "\n",
        "for n in subject_id:\n",
        "      print()\n",
        "      print()\n",
        "    \n",
        "      # Load data\n",
        "      y_train = np.load(data_path+'/y_train_'+str(n)+'.npy')\n",
        "      X_train = np.load(data_path+'/X_train_'+str(n)+'.npy')\n",
        "\n",
        "\n",
        "\n",
        "      trials, dim1, dim2 = X_train.shape\n",
        "\n",
        "      # print(stop)\n",
        "\n",
        "      if (dim1 == num_channels):\n",
        "          # chans = dim1, samples = dim2\n",
        "          X_train = np.reshape(X_train, (trials, 1 , dim1, dim2))\n",
        "      elif (dim1 == num_samples):\n",
        "          # samples = dim1, chans = dim2\n",
        "          X_train = np.transpose(X_train, (0, 2, 1))\n",
        "          X_train = np.reshape(X_train, (trials, 1, dim2, dim1))\n",
        "          X_train= np.array(X_train)\n",
        "      else:\n",
        "          print(\"Error\")\n",
        "      print(X_train.shape)\n",
        "      \n",
        "      y_val = np.load(data_path+'/y_val_'+str(n)+\".npy\")\n",
        "      X_val = np.load(data_path+'/X_val_'+str(n)+\".npy\")\n",
        "      trials, dim1, dim2 = X_val.shape\n",
        "      if (dim1 == num_channels):\n",
        "          # chans = dim1, samples = dim2\n",
        "          X_val = np.reshape(X_val, (trials, 1 , dim1, dim2))\n",
        "      elif (dim1 == num_samples):\n",
        "          # samples = dim1, chans = dim2\n",
        "          X_val = np.transpose(X_val, (0, 2, 1))\n",
        "          X_val = np.reshape(X_val, (trials, 1, dim2, dim1))\n",
        "      else:\n",
        "          print(\"Error\")\n",
        "      print(X_val.shape)\n",
        "      \n",
        "      # Build Model\n",
        "      model = EEGNet(Samples = num_samples)\n",
        "      model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['acc'])\n",
        "      model.summary()\n",
        "\n",
        "      # Add Checkpoint\n",
        "      checkpointer = ModelCheckpoint(filepath= output_path+\"/subject\"+str(n), \n",
        "                                    monitor='val_acc',\n",
        "                                    mode='max',\n",
        "                                    verbose=1,\n",
        "                                    save_best_only=True)\n",
        "      print(\"X_train: \", X_train.shape)\n",
        "      print(\"y_train: \", y_train.shape)\n",
        "      print(\"X_val: \", X_val.shape)\n",
        "      print(\"y_val: \", y_val.shape)\n",
        "      # Train and save model\n",
        "      history = model.fit(X_train,\n",
        "                          y_train,\n",
        "                          batch_size=16,\n",
        "                          validation_data=(X_val, y_val),\n",
        "                          epochs=500,\n",
        "                          callbacks=[checkpointer])\n",
        "\n",
        "      np.save(output_path+\"/subject\"+str(n)+\"_acc\", history.history[\"acc\"])\n",
        "      np.save(output_path+\"/subject\"+str(n)+\"_loss\", history.history[\"loss\"])\n",
        "      np.save(output_path+\"/subject\"+str(n)+\"_val_acc\", history.history[\"val_acc\"])\n",
        "      np.save(output_path+\"/subject\"+str(n)+\"_val_loss\", history.history[\"val_loss\"])\n",
        "        \n",
        "      print()\n",
        "      print()\n",
        "    \n",
        "print()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoVOjQ_qQgs9",
        "outputId": "7eac7649-f0b9-42a9-b09f-b77d580c49f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6007 - accuracy: 0.7759\n",
            "Subject  0 : 77.59%\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 1.2014 - accuracy: 0.5000\n",
            "Subject  1 : 50.00%\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.5967 - accuracy: 0.7759\n",
            "Subject  2 : 77.59%\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.9089 - accuracy: 0.6379\n",
            "Subject  3 : 63.79%\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.9523 - accuracy: 0.6724\n",
            "Subject  4 : 67.24%\n",
            "\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1571 - accuracy: 0.4483\n",
            "Subject  5 : 44.83%\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.6982 - accuracy: 0.6034\n",
            "Subject  6 : 60.34%\n",
            "\n",
            "2/2 [==============================] - 0s 5ms/step - loss: 0.8273 - accuracy: 0.6724\n",
            "Subject  7 : 67.24%\n",
            "\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3094 - accuracy: 0.9138\n",
            "Subject  8 : 91.38%\n",
            "\n",
            "\n",
            "Mean Accuracy of EEGNet is 66.66666666666666 %\n",
            "Standard Deviation of EEGNet is 13.55152438629922 %\n"
          ]
        }
      ],
      "source": [
        "#TestEEG\n",
        "import numpy as np\n",
        "import h5py\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# channels first\n",
        "tf.keras.backend.set_image_data_format('channels_first')\n",
        "\n",
        "\n",
        "# Define variables\n",
        "subject_id = np.arange(0,9)      # 9 subjects \n",
        "#subject_id = [4]\n",
        "num_channels = 22                 # 22 channels\n",
        "tot_samples = 1000                # 1000 total samples \n",
        "fs = 250                          # sampling rate 250Hz\n",
        "\n",
        "# define file paths\n",
        "data_path = '/content/drive/MyDrive/MotorImagery-master/Data/PublicData/500Windowed/0'\n",
        "\n",
        "num_samples = 500\n",
        "accuracy=[]\n",
        "\n",
        "# loop through subjects \n",
        "for n in subject_id:\n",
        "      weights_path = '/content/drive/MyDrive/MotorImagery-master/Data/PublicData/500Windowed/0/EEG_train'\n",
        "      # load testing data\n",
        "      X_test = np.load(data_path+'/X_test_'+str(n)+'.npy')\n",
        "      y_test = np.load(data_path+'/y_test_'+str(n)+'.npy')\n",
        "      # Reshape Xtest data into (trials, 1, chans, samples)\n",
        "      trials, dim1, dim2 = X_test.shape\n",
        "      if (dim1 == num_channels):\n",
        "          # chans = dim1, samples = dim2\n",
        "          X_test = np.reshape(X_test, (trials, 1 , dim1, dim2))\n",
        "      elif (dim1 == num_samples):\n",
        "          # samples = dim1, chans = dim2\n",
        "          X_test = np.transpose(X_test, (0, 2, 1))\n",
        "          X_test = np.reshape(X_test, (trials, 1, dim2, dim1))\n",
        "      else:\n",
        "          print(\"Error\")\n",
        "          \n",
        "      # Build Model\n",
        "      model = EEGNet(Samples = num_samples)\n",
        "      model.compile(optimizer='adam',\n",
        "                    loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "      \n",
        "      # Load trained model\n",
        "      model.load_weights(weights_path+\"/subject\"+str(n))\n",
        "              \n",
        "      # Evaluate\n",
        "      loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
        "      accuracy.append(acc)\n",
        "      print(\"Subject \", n, \": {:5.2f}%\".format(100*acc))\n",
        "      print()\n",
        "\n",
        "      #Predict\n",
        "      pred_array = model.predict(X_test)\n",
        "      #print(pred_array)\n",
        "\n",
        "      # make output to csv\n",
        "\n",
        "print()\n",
        "print(\"Mean Accuracy of EEGNet is\",np.mean(accuracy)*100,\"%\")\n",
        "print(\"Standard Deviation of EEGNet is\",np.std(accuracy)*100,\"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU4lRl5L7E-D"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}